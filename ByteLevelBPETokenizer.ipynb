{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ByteLevelBPETokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "67Hy4oo0GP0Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "e2195fb0-36a1-4817-e2bc-98f3604f798d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/Tweet Sentiment Extraction Final"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/Tweet Sentiment Extraction Final\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcMr2J_PF9-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "ffade0f5-66d2-4332-f368-5c6e8230089e"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.0rc4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEuRT6a4FBO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/huggingface/tokenizers\n",
        "import tokenizers\n",
        "\n",
        "class config:\n",
        "  MAX_LEN = 96\n",
        "  TOKENIZER_PATH = './input/roberta-tokenizer'\n",
        "  TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab_file=f'{TOKENIZER_PATH}/vocab.json',\n",
        "    merges_file=f'{TOKENIZER_PATH}/merges.txt',\n",
        "    lowercase=True,\n",
        "    add_prefix_space=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtiFTkVWFfLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweet='The quick brown fox jumps over the lazy dog'\n",
        "selected_text='jumps over the lazy dog'\n",
        "sentiment='positive'\n",
        "tokenizer=config.TOKENIZER\n",
        "max_len=config.MAX_LEN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHB6EVxfFfJU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bab5b2e2-8c96-4822-eda2-214e2962c311"
      },
      "source": [
        "# For Roberta, CLS = <s> and SEP = </s>\n",
        "# <s>The quick brown fox jumps over the lazy dog</s></s>positive</s>'  \n",
        "# Convert tweet to string just to ensure that nothing breaks in the future\n",
        "# Next split on all kinds of spaces to remove superfluous spaces that we might have \n",
        "# Repeating the same with selected_text\n",
        "tweet = \" \" + \" \".join(str(tweet).split())\n",
        "selected_text = \" \" + \" \".join(str(selected_text).split())\n",
        "print(tweet)\n",
        "print(selected_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " The quick brown fox jumps over the lazy dog\n",
            " jumps over the lazy dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6dpUhs8FfGr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "204cde08-c893-43a4-aeaa-649bf2ffa999"
      },
      "source": [
        "# Get selected_text start and end idx\n",
        "len_st = len(selected_text) - 1\n",
        "idx0 = None\n",
        "idx1 = None\n",
        "\n",
        "for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
        "    if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
        "        idx0 = ind\n",
        "        idx1 = ind + len_st - 1\n",
        "        break\n",
        "\n",
        "print(f\"idx0: {idx0}\")\n",
        "print(f\"idx1: {idx1}\")\n",
        "print(f\"len_st: {len_st}\")\n",
        "print(f\"idxed tweet: {tweet[idx0: idx1+1]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "idx0: 21\n",
            "idx1: 43\n",
            "len_st: 23\n",
            "idxed tweet: jumps over the lazy dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwtvqpVrFfER",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "66224388-7114-49d4-d373-0949de6d54ba"
      },
      "source": [
        "# Assign 1 as target for each char in selected_text\n",
        "char_targets = [0] * len(tweet)\n",
        "if idx0 != None and idx1 != None:\n",
        "    for ct in range(idx0, idx1 + 1):\n",
        "        char_targets[ct] = 1\n",
        "\n",
        "print(f\"char_targets: {char_targets}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "char_targets: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZprDrDyIlpx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "62b45c31-c3ac-4ba3-b1f4-224821c46c79"
      },
      "source": [
        "# For RoBERTa, tokenizer.encode doesn't add any special tokens \n",
        "# Hence no need to remove anything from tokenized_tweet.ids & tokenized_tweet.offsets\n",
        "tokenized_tweet = tokenizer.encode(tweet)\n",
        "input_ids_original = tokenized_tweet.ids\n",
        "tweet_offsets = tokenized_tweet.offsets\n",
        "\n",
        "print(tweet)\n",
        "print(selected_text)\n",
        "print(input_ids_original)\n",
        "print(f\"offsets: {tweet_offsets}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " The quick brown fox jumps over the lazy dog\n",
            " jumps over the lazy dog\n",
            "[5, 2119, 6219, 23602, 13855, 81, 5, 22414, 2335]\n",
            "offsets: [(0, 4), (4, 10), (10, 16), (16, 20), (20, 26), (26, 31), (31, 35), (35, 40), (40, 44)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5G1E5XXInqL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "0ea623d5-e625-4851-f107-c80926c24922"
      },
      "source": [
        "# Get ids within tweet of words that have target char\n",
        "target_idx = []\n",
        "for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
        "    if sum(char_targets[offset1: offset2]) > 0:\n",
        "        target_idx.append(j)\n",
        "\n",
        "targets_start = target_idx[0]\n",
        "targets_end = target_idx[-1]\n",
        "\n",
        "print(f\"target_idx= {target_idx}\")\n",
        "\n",
        "print(targets_start)\n",
        "print(targets_end)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "target_idx= [4, 5, 6, 7, 8]\n",
            "4\n",
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT9fvr9EIpdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "7be66804-24af-435d-fb38-6b4af90f31b7"
      },
      "source": [
        "# Hard-coded values\n",
        "sentiment_id = {\n",
        "        'positive': 1313,\n",
        "        'negative': 2430,\n",
        "        'neutral': 7974\n",
        "    }\n",
        "\n",
        "# id for <s>: 0\n",
        "# id for </s>: 2\n",
        "# Input for RoBERTa\n",
        "input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_original + [2]\n",
        "token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_original) + 1) # RoBERTa doesn't care about token_type_ids \n",
        "mask = [1] * len(token_type_ids)\n",
        "tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n",
        "targets_start += 4\n",
        "targets_end += 4\n",
        "\n",
        "print(input_ids)\n",
        "print(token_type_ids)\n",
        "print(mask)\n",
        "print(tweet_offsets)\n",
        "print(targets_start)\n",
        "print(targets_end)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1313, 2, 2, 5, 2119, 6219, 23602, 13855, 81, 5, 22414, 2335, 2]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[(0, 0), (0, 0), (0, 0), (0, 0), (0, 4), (4, 10), (10, 16), (16, 20), (20, 26), (26, 31), (31, 35), (35, 40), (40, 44), (0, 0)]\n",
            "8\n",
            "12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO7ExhIXIra3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "1bd8641f-d626-4043-af40-bfce1deda32c"
      },
      "source": [
        "# Input padding: new mask, token type ids, tweet offsets\n",
        "padding_length = max_len - len(input_ids)\n",
        "if padding_length > 0:\n",
        "    input_ids = input_ids + ([1] * padding_length)\n",
        "    mask = mask + ([0] * padding_length)\n",
        "    token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "    tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
        "    \n",
        "print(input_ids)\n",
        "print(token_type_ids)\n",
        "print(mask)\n",
        "print(tweet_offsets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1313, 2, 2, 5, 2119, 6219, 23602, 13855, 81, 5, 22414, 2335, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[(0, 0), (0, 0), (0, 0), (0, 0), (0, 4), (4, 10), (10, 16), (16, 20), (20, 26), (26, 31), (31, 35), (35, 40), (40, 44), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}